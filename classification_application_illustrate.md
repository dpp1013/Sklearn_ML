#### 数据说明

| 1 | 2 | 3-15 | 16-18 | 29-41 |
| :-----| :---- | :---- | :----|:---- |
| 时间戳 | 心率 | 传感器1 | 传感器2 |传感器3|
####KNN
说明:通过计算待分类数据点,与已有数据集中的所有数据点的距离.取距离
最小的前k个点,根据"少数服从多数"的原则,将这个数据点划分为出现次数最多
的那个类别.

from sklearn.neighbors import KNeighborsClassifier

参数:

- n_neighbors:分类器中k的大小,默认为5
- weights:设置选中k个点对分类结果影响的权重(默认为平均权重'uniform',可以选择'distance'代表越近的点权重
越高,或者传入自己编写的以距离为参数的权重计算函数)

####KNN的使用经验

在实际使用中,我们可以使用所有训练数据构成特征X和标签y,使用fit()函数进行
训练.在正式分类时,通过一次性构造测试集或者一个一个输入样本的方式,得到
样本对应的分类结构:

####关于K的取值:

- 如果较大,相当于使用较大领域中的训练实例进行预测,可以减小估计误差,但是距离
较远的样本也会对预测起作用,导致预测错误.
- 相反地,如果k较小,相当于使用较小的领域进行预测,如果邻居恰好是噪声点,会导致
过拟合
- 一般情况下,k会倾向于选取较小的值,并使用交叉验证法选取最优k值
####决策树

决策树是一种树形结构的分类器,通过顺序询问分类点的属性决定分类点最终的类别.通常
根据特征的信息增益或其他指标,构建一颗决策树.在分类时,只需要按照决策树中的节点依次
进行判断,即可得到样本所属类别.

决策树本质上是寻找一种对特征空间上的划分,旨在构建一个训练数据拟合的好,
并且复杂度小的决策树.

实际使用中,需要根据数据情况,调整DecisionTreeClassificater类中传入
参数,比如选择合适的criterion,设置随机变量等.

from sklearn import DecisionTreeClassifier

####参数
- criterion:用于选择属性的准则,可以传入'gini'代表基尼系数,或者'entropy'代表信息增益
- max_features:表示在决策树结点进行分类时,从多少个特征中选择最优特征.可以设定固定数目,百分比或其他标准.
它的默认值是使用所有特征个数

#### 朴素贝叶斯

对于给定数据,首先基于特征的条件独立性假设,学习输入输出的联合概率分布,然后基于此模型,
对给定的输入x,利用贝叶斯定理求出后验概率最大的输出y.

|分类器|描述|
|----|----|
naive_bayes.GussianNB|高斯朴素贝叶斯
naive_bayes.GussianNB|针对多项式模型的朴素贝叶斯分类
naive_bayes.GussianNB|针对多元伯努利模型的朴素贝叶斯分类器

区别: 假设某一特征的所有属于某个类别的观测值符合特定分布,如,分类问题的
特征包括人的身高,身高符合高斯分布,这类问题适合高斯朴素贝叶斯.

<-- sklearn.naive_bayes.Gaussian -->